{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Calculus of Maximum Likelihood",
      "provenance": [],
      "collapsed_sections": [
        "QKzJ8ddAodl5",
        "yK1jMPdKoSL7",
        "-5XVTYHTXjiV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq1cwLdiFQ2b",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "!pip install -q symbulate\n",
        "from symbulate import *\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPXJ7aMCc-bQ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define Plotting Functions\n",
        "\n",
        "def plot_discrete_function(f, xlim=(0, 10), xlabel=r\"$\\theta$\", ylabel=\"Likelihood\"):\n",
        "  xs = np.arange(np.ceil(xlim[0]), np.floor(xlim[1]) + 1, dtype=int)\n",
        "  ys = [f(x) for x in xs]\n",
        "  plt.plot(xs, ys, \"o-\")\n",
        "  plt.xlabel(xlabel, fontsize=18)\n",
        "  plt.ylabel(ylabel, fontsize=18)\n",
        "  plt.xlim(*xlim)\n",
        "\n",
        "def plot_continuous_function(f, xlim=(0, 1), xlabel=r\"$\\theta$\", ylabel=\"Likelihood\"):\n",
        "  xs = np.linspace(xlim[0], xlim[1], 1000)\n",
        "  ys = [f(x) for x in xs]\n",
        "  plt.plot(xs, ys, \"-\")\n",
        "  plt.xlabel(xlabel, fontsize=18)\n",
        "  plt.ylabel(ylabel, fontsize=18)\n",
        "  plt.xlim(*xlim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjRZd_9YFaEA",
        "colab_type": "text"
      },
      "source": [
        "# The Calculus of Maximum Likelihood\n",
        "\n",
        "\n",
        "The number of radioactive particles that reach a Geiger counter (per minute) is a $\\text{Poisson}(\\mu)$ random variable. Suppose that over a period of one minute, we record 7 radioactive particles hitting the Geiger counter. What is the MLE of $\\mu$?\n",
        "\n",
        "The likelihood is \n",
        "$$ L(\\mu) = e^{-\\mu} \\frac{\\mu^7}{7!}. $$\n",
        "\n",
        "The maximum likelihood estimate (MLE) $\\hat\\mu$ is the value of $\\mu$ that maximizes this function. In mathematical notation, \n",
        "$$ \\hat\\mu = \\underset{\\mu}{\\arg\\max}\\ L(\\mu). $$\n",
        "\n",
        "So far, we've solved for $\\hat\\mu$ by taking the derivative and setting it equal to $0$, i.e.,\n",
        "$$ L'(\\mu) = 0. $$\n",
        "The derivative is a mess because we have a _product_ of terms involving $\\mu$ (i.e., $e^{-\\mu}$ and $\\mu^7$). We have to apply the product rule from calculus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqedzQHNnuYY",
        "colab_type": "text"
      },
      "source": [
        "## Transforming the Likelihood\n",
        "\n",
        "Instead of maximizing $L(\\mu)$ we can instead maximize $g(L(\\mu))$, where $g$ is _any_ monotone increasing function. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKzJ8ddAodl5",
        "colab_type": "text"
      },
      "source": [
        "### Definition (Monotone Increasing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yszMEJlolz9",
        "colab_type": "text"
      },
      "source": [
        "A function $g(x)$ is said to be **monotone increasing** function if for any two values $x_1 < x_2$, we have $g(x_1) < g(x_2)$.\n",
        "\n",
        "In other words, $g$ always increases as you move from left to right. Examples of monotone increasing functions include $g(x) = \\log(x)$, $g(x) = x^3$, and $g(x) = x$. Functions like $x^2$ and $\\sin(x)$ are not monotone increasing because they sometimes go up and sometimes go down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaJafZKUFfZe",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Graph Monotone Increasing Functions\n",
        "\n",
        "plot_continuous_function(log, xlim=(1e-4, 4),\n",
        "                         xlabel=\"$x$\", ylabel=\"$g(x)$\")\n",
        "plot_continuous_function(lambda x: x ** 3, xlim=(-2, 2),\n",
        "                         xlabel=\"$x$\", ylabel=\"$g(x)$\")\n",
        "plot_continuous_function(lambda x: x, xlim=(-2, 2),\n",
        "                         xlabel=\"$x$\", ylabel=\"$g(x)$\")\n",
        "\n",
        "plt.legend([r\"$\\log(x)$\", r\"$x^3$\", r\"$x$\"])\n",
        "plt.ylim(-6, 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxWTGtEif50o",
        "colab_type": "text"
      },
      "source": [
        "### Theorem (Monotone Transformations of the Likelihood)\n",
        "\n",
        "Let $L$ be any function and $g$ be a monotone increasing function. If $\\hat\\theta$ maximizes $g(L(\\theta))$, then $\\hat\\theta$ also maximizes $L(\\theta)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1jMPdKoSL7",
        "colab_type": "text"
      },
      "source": [
        "#### Proof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD2JeDgnoKyb",
        "colab_type": "text"
      },
      "source": [
        "In order for $\\hat\\theta$ to maximize $L$, we need to show that $L(\\hat\\theta) \\geq L(\\theta^*)$ for all values $\\theta^*$.\n",
        "\n",
        "But we know that $\\hat\\theta$ maximizes $g(L(\\theta))$. So we know that $g(L(\\hat\\theta)) \\geq g(L(\\theta^*))$ for all values $\\theta^*$. \n",
        "\n",
        "Now, suppose by contradiction that $\\hat\\theta$ did _not_ maximize $L$. That would mean that $L(\\hat\\theta) < L(\\theta^*)$ for some $\\theta^*$. Since $g$ is monotone increasing, that would mean that $g(L(\\hat\\theta)) < g(L(\\theta^*))$ for that $\\theta^*$. But that contradicts the statement above that $g(L(\\hat\\theta)) \\geq g(L(\\theta^*))$ for all $\\theta^*$. So $\\hat\\theta$ must maximize $L$.\n",
        "\n",
        "_Q.E.D._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7kyNGVMl40z",
        "colab_type": "text"
      },
      "source": [
        "### Which Monotone Transformation to Choose?\n",
        "\n",
        "We choose a monotone increasing function that makes it easy to take derivatives. It turns out that taking the logarithm is usually the best transformation:\n",
        "$$ \\ell(\\theta) \\overset{\\text{def}}{=} \\log L(\\theta). $$\n",
        "\n",
        "Notice that $\\log$ here is the _natural logarithm_ (which some people notate as $\\ln$). In most science and engineering fields, base $e$ is the default, not base $10$. If you don't believe me, just check out what Python does by default:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30yy9rOOgRy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log(e), log(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USQ5fQEvhcXV",
        "colab_type": "text"
      },
      "source": [
        "Now let's plot the likelihood and the log-likelihood for the Poisson example above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyBbhHB7e9zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def likelihood(mu):\n",
        "  return Poisson(mu).pmf(7)\n",
        "\n",
        "def log_likelihood(mu):\n",
        "  return log(likelihood(mu))\n",
        "  \n",
        "plot_continuous_function(likelihood, xlim=(1, 15),\n",
        "                         xlabel=\"$\\mu$\", ylabel=\"\")\n",
        "plot_continuous_function(log_likelihood, xlim=(1, 15),\n",
        "                         xlabel=\"$\\mu$\", ylabel=\"\")\n",
        "\n",
        "plt.ylim(-4, 0.4)\n",
        "plt.legend([r\"$L(\\mu)$\", r\"$\\log(L(\\mu))$\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifqX5t7ceuZz",
        "colab_type": "text"
      },
      "source": [
        "Notice that the likelihood and the log-likelihood are very different functions. However, they both achieve their maximum at the same value of $\\mu$. So maximizing one is equivalent to maximizing the other.\n",
        "\n",
        "Besides making the calculus easier (as we will see in a second), the graph above also illustrates another benefits of taking logs. The values of the likelihood are very small numbers. Taking logs also has the benefit of spreading out those very small numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUBdDhvwd74D",
        "colab_type": "text"
      },
      "source": [
        "## Maximizing the Log-Likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysohWn1rr6pB",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Calculate the Log-Likelihood\n",
        "\n",
        "Now let's try maximizing the log-likelihood. First, we calculate and simplify the log-likelihood using properties of logarithms:\n",
        "\n",
        "\\begin{align}\n",
        "\\ell(\\mu) = \\log L(\\mu) &= \\log \\left( e^{-\\mu} \\frac{\\mu^7}{7!} \\right) \\\\\n",
        "&= \\log e^{-\\mu} + \\log \\mu^7 - \\log(7!) \\\\\n",
        "&= -\\mu + 7 \\log \\mu - \\log(7!)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5XVTYHTXjiV",
        "colab_type": "text"
      },
      "source": [
        "### Refresher on the Properties of Logarithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWM1IObZpv6c",
        "colab_type": "text"
      },
      "source": [
        "1. $\\log(ab) = \\log(a) + \\log(b)$\n",
        "2. $\\log(a^b) = b \\log(a)$\n",
        "3. $\\log(a / b) = \\log(a) - \\log(b)$ (Actually, this property follows from properties 1 and 2.)\n",
        "\n",
        "Also, it is useful to know the derivative of the (natural) log:\n",
        "$$ \\frac{d}{dx} \\log(x) = \\frac{1}{x}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgWc5yJkXkNJ",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Take the Derivative and Set it Equal to Zero\n",
        "\n",
        "We showed above that the log-likelihood is \n",
        "$$\\ell(\\mu) = -\\mu + 7 \\log \\mu - \\log(7!). $$\n",
        "\n",
        "Now we take the derivative of the log-likelihood and set it equal to 0.\n",
        "\n",
        "\\begin{align}\n",
        "\\ell'(\\mu) &= -1 + \\frac{7}{\\mu} - 0 = 0\n",
        "\\end{align}\n",
        "\n",
        "Solving for $\\mu$, we see that the MLE is $\\hat\\mu = 7$.\n",
        "\n",
        "By the way, the _derivative of the log-likelihood_ $\\ell'$ turns out to be a very important quantity in mathematical statistics. It is called the **score**, and the equation $\\ell'(\\mu) = 0$ is called the **score equation**. You do not have to worry about it now, but it will play an important role later in this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tycfrmRWBLgP",
        "colab_type": "text"
      },
      "source": [
        "## Maximum Likelihood Estimator (as a function of $X$)\n",
        "\n",
        "We calculated the MLE of $\\mu$ when $X = 7$. What if $X$ had been $5$? We would have to go through the entire process again: writing down the likelihood (for $X=5$), taking the log, setting the derivative equal to 0, and solving for $\\mu$.\n",
        "\n",
        "However, if we simply leave $X$ in the likelihood, then we will get a formula for the MLE in terms of $X$. This is an _estimator_ because we can say what the estimate will be for any value of $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy-7PA_sCDgl",
        "colab_type": "text"
      },
      "source": [
        "Let's start by leaving the random variable $X$ in the likelihood:\n",
        "\n",
        "$$ L(\\mu) = e^{-\\mu} \\frac{\\mu^X}{X!}. $$\n",
        "\n",
        "Next, we calculate the log-likelihood:\n",
        "\n",
        "$$ \\ell(\\mu) = \\log L(\\mu) = -\\mu + X \\log \\mu - \\log(X!). $$\n",
        "\n",
        "Now, we set the derivative equal to 0. Be careful here! Although it may be tempting to take the derivative with respect to $X$, remember that the variable here is $\\mu$. \n",
        "\n",
        "$$ \\frac{\\partial\\ell}{\\partial\\mu} = -1 + \\frac{X}{\\mu} - 0 = 0. $$\n",
        "\n",
        "Finally, solving for $\\mu$, we see that \n",
        "\n",
        "$$ \\hat\\mu = X. $$\n",
        "\n",
        "So the maximum likelihood estimator for the Poisson distribution can be described as follows: however many radioactive particles $X$ we record in a single minute, that number is our estimate for $\\mu$."
      ]
    }
  ]
}