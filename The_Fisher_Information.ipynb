{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Fisher Information",
      "provenance": [],
      "collapsed_sections": [
        "OsPJmOG3YOzZ",
        "YgGPt_7yY8_t",
        "q1huLywsRun6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlsun/Stat425F19/blob/master/The_Fisher_Information.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVUjNTCTdikV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Import Symbulate\n",
        "!pip install -q symbulate\n",
        "from symbulate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd7_JJg2dNWS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define Plotting Functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_continuous_function(f, xlim=(0, 1), \n",
        "                             xlabel=r\"$\\theta$\", \n",
        "                             ylabel=\"Log Likelihood $\\ell$\"):\n",
        "  xs = np.linspace(xlim[0], xlim[1], 1000)\n",
        "  ys = [f(x) for x in xs]\n",
        "  plt.plot(xs, ys, \"-\")\n",
        "  plt.xlabel(xlabel, fontsize=18)\n",
        "  plt.ylabel(ylabel, fontsize=18)\n",
        "  plt.xlim(*xlim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsgcRFDeVjP7",
        "colab_type": "text"
      },
      "source": [
        "# The Fisher Information\n",
        "\n",
        "$\n",
        "\\def\\Var{\\text{Var}}\n",
        "\\def\\Cov{\\text{Cov}}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq_yHI7kdEdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate some data\n",
        "X = RV(Normal(0, 1) ** 10)\n",
        "x = X.draw()\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihOoHMtJdLml",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Plot Log Likelihoods\n",
        "\n",
        "# Define the log-likelihood of all 10 observations\n",
        "def log_likelihood(mu):\n",
        "  n = len(x)\n",
        "  return -n * log(sqrt(2 * pi)) - sum((x - mu) ** 2 / 2)\n",
        "\n",
        "# Define the log-likelihood of just the first 2 observations\n",
        "def log_likelihood_2(mu):\n",
        "  n = 2\n",
        "  return -n * log(sqrt(2 * pi)) - sum((x[:2] - mu) ** 2 / 2)\n",
        "\n",
        "# Plot the two log-likelihoods\n",
        "plot_continuous_function(log_likelihood, xlim=(-3, 3))\n",
        "plot_continuous_function(log_likelihood_2, xlim=(-3, 3))\n",
        "\n",
        "plt.legend([\"$n = 10$\", \"$n = 2$\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQUg-twHfHYQ",
        "colab_type": "text"
      },
      "source": [
        "How would you compare the blue log-likelihood (based on $n=10$ observations) to the green log-likelihood (based on $n=2$ observations)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMWr2IZ4fh7F",
        "colab_type": "text"
      },
      "source": [
        "The log-likelihood with more data is more \"curved\". The more curved the log-likelihood is, the more precise the MLE is.\n",
        "\n",
        "In mathematics, curvature is measured using the second derivative. That is, $\\ell''$ contains information about the precision of our estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsPJmOG3YOzZ",
        "colab_type": "text"
      },
      "source": [
        "## Fisher Information\n",
        "\n",
        "The **Fisher information** measures the amount of information that data $X$ contains about the parameter $\\theta$. It is defined as \n",
        "\n",
        "$$ I(\\theta) \\overset{\\text{def}}{=} E[-\\ell''(\\theta)]. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoTg2ym9kTTC",
        "colab_type": "text"
      },
      "source": [
        "Note the negative sign. This is because the log-likelihood $\\ell$ is usually concave down, so $\\ell''$ is negative. To make the measure of curvature positive, we tack on a negative sign in the definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgGPt_7yY8_t",
        "colab_type": "text"
      },
      "source": [
        "## Cramer-Rao Inequality (a.k.a. Information Inequality)\n",
        "\n",
        "We have said that among unbiased estimators, we prefer the one with the smallest variance. The **Cramer-Rao Inequality** provides a lower-bound on the variance of _any_ unbiased estimator.\n",
        "\n",
        "Let $T(X)$ be any unbiased estimator of $\\theta$. Then \n",
        "$$ \\Var_\\theta[T(X)] \\geq \\frac{1}{I(\\theta)}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_n7l4yekW5e",
        "colab_type": "text"
      },
      "source": [
        "We will prove this theorem next time. Today, we will focus on how to apply this theorem.\n",
        "\n",
        "If we can find an unbiased estimator that has $\\frac{1}{I(\\theta)}$ as its variance, then we can be certain that we have found the best possible unbiased estimator. This estimator is called **efficient**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1huLywsRun6",
        "colab_type": "text"
      },
      "source": [
        "# Example\n",
        "\n",
        "Let $X$ be $\\text{Binomial}(n, p)$. Show that there is no unbiased estimator of $p$ that is better than the MLE $\\hat p = X / n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogmqwXfaUCan",
        "colab_type": "text"
      },
      "source": [
        "The log-likelihood is \n",
        "$$ \\ell(p) = \\log\\binom{100}{X} + X \\log p + (n - X) \\log (1 - p). $$\n",
        "The score is \n",
        "$$ \\ell'(p) = \\frac{X}{p} - \\frac{n-X}{1-p}. $$\n",
        "Taking another derivative, we get \n",
        "$$ \\ell''(p) = -\\frac{X}{p^2} - \\frac{n-X}{(1-p)^2}. $$ \n",
        "The Fisher information is \n",
        "\\begin{align}\n",
        "I(p) = E[-\\ell''(p)] &= \\frac{E[X]}{p^2} + \\frac{n-E[X]}{(1-p)^2} \\\\\n",
        "&= \\frac{np}{p^2} + \\frac{n-np}{(1-p)^2} \\\\\n",
        "&= \\frac{n}{p} + \\frac{n}{1-p} \\\\\n",
        "&= \\frac{n}{p(1-p)}.\n",
        "\\end{align}\n",
        "\n",
        "The Cramer-Rao inequality says that no unbiased estimator can have a variance lower than \n",
        "$$ \\frac{1}{I(p)} = \\frac{p(1-p)}{n}. $$\n",
        "\n",
        "Notice that $\\hat p$ is unbiased, with variance equal to \n",
        "$$ \\Var[\\hat p] = \\Var[\\frac{X}{n}] = \\frac{1}{n^2} np(1-p) = \\frac{p(1-p)}{n}, $$\n",
        "so it actually achieves the minimum possible variance."
      ]
    }
  ]
}