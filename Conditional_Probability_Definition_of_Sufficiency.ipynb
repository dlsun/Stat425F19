{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditional Probability Definition of Sufficiency",
      "provenance": [],
      "collapsed_sections": [
        "0-Gf_XKMrJrk",
        "pviwiXl5ur-5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MkEt4Rwqmy3",
        "colab_type": "text"
      },
      "source": [
        "# Conditional Probability Definition of Sufficiency\n",
        "\n",
        "Last lecture, we said that if the joint probability distribution could be factored as \n",
        "\n",
        "$$ p_\\theta(x_1, ..., x_n) = g(T(x_1, ..., x_n), \\theta) \\cdot h(x_1, ..., x_n), $$\n",
        "\n",
        "then $T(x_1, ..., x_n)$ is called a **sufficient statistic**. We saw that the MLE only depends on the value of the sufficient statistic, so we do not need the raw data if we know the sufficient statistic.\n",
        "\n",
        "In other words, the sufficient statistic $T$ contains all the information in the data for estimating the parameter $\\theta$. We've seen that one way to quantify information is through _conditional probability_. \n",
        "\n",
        "The following definition of sufficiency, an alternative to the factorization definition above, is based on conditional probability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_k6IfZWFbip",
        "colab_type": "text"
      },
      "source": [
        "## Definition (Conditional Probability Definition of Sufficiency)\n",
        "\n",
        "$T$ is a sufficient statistic for $\\theta$ if the conditional distribution of the raw data, given $T$,\n",
        "\n",
        "$$P(X_1 = x_1 \\cap ... \\cap X_n = x_n | T = t)$$\n",
        "\n",
        "does not depend on $\\theta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Gf_XKMrJrk",
        "colab_type": "text"
      },
      "source": [
        "## Example\n",
        "\n",
        "Let $X_1, ..., X_n$ be i.i.d. Bernoulli trials with probability $p$ (i.e., $\\text{Binomial}(n=1, p)$). Show that $T = \\sum_{i=1}^n X_i$ is sufficient for $p$ using the conditional probability definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "62WE0YRdtKat"
      },
      "source": [
        "First, we write out the definition of conditional probability:\n",
        "\n",
        "\\begin{align}\n",
        "P(X_1 = x_1 \\cap \\cdots \\cap X_n = x_n | T=t) &= \\frac{P(X_1=x_1 \\cap \\cdots \\cap X_n=x_n \\cap T=t)}{P(T=t)}\n",
        "\\end{align}\n",
        "\n",
        "In the numerator, observe that the probability is 0, unless $t = \\sum x_i$, in which case the event $T = t$ is redundant given the other events. So we can write the probability casewise as \n",
        "\n",
        "$$ \\begin{cases} \\displaystyle\\frac{P(X_1 = x_1 \\cap \\cdots \\cap X_n = x_n)}{P(T = t)} & t = \\sum x_i \\\\ 0 & \\text{otherwise} \\end{cases}. $$\n",
        "\n",
        "In the numerator, $X_1, ..., X_n$ are i.i.d. $\\text{Binomial}(n=1, p)$. In the denominator, $T = \\sum_{i=1}^n X_i$ is $\\text{Binomial}(n, p)$. Plugging in these p.m.f.s, we obtain \n",
        "\n",
        "$$ \\begin{cases} \\displaystyle\\frac{\\prod_{i=1}^n p^{x_i} (1 - p)^{1-x_i}}{\\binom{n}{t} p^t (1 - p)^{n-t}} & t = \\sum x_i \\\\ 0 & \\text{otherwise} \\end{cases}. $$\n",
        "\n",
        "The numerator simplifies to $p^t (1 - p)^{n-t}$ and cancels out with the denominator. That leaves us with \n",
        "\n",
        "$$ P(X_1 = x_1 \\cap \\cdots \\cap X_n = x_n | T=t) = \\begin{cases} \\displaystyle\\frac{1}{\\binom{n}{t}} & t = \\sum x_i \\\\ 0 & \\text{otherwise} \\end{cases}. $$\n",
        "\n",
        "Note that all instances of the parameter $p$ have dropped out of the conditional probability! Therefore, by the conditional distribution definition of sufficiency, $T$ is sufficient for $p$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEbGKnGVuGtC",
        "colab_type": "text"
      },
      "source": [
        "# Equivalence of the Definitions of Sufficiency\n",
        "\n",
        "We have presented two definitions of sufficiency: one based on factorization and another based on conditional probability. We need to check that these two definitions are equivalent.\n",
        "\n",
        "To simplify notation, we will use the notation $\\vec x = (x_1, ..., x_n)$ to represent the $n$ observations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pviwiXl5ur-5",
        "colab_type": "text"
      },
      "source": [
        "## Fisher-Neyman Factorization Theorem\n",
        "\n",
        "Let $p_\\theta(\\vec x)$ be any p.m.f. Then, $p_\\theta$ has a factorization of the form\n",
        "\n",
        "$$ p_\\theta(\\vec x) = g(T(\\vec x), \\theta) \\cdot h(\\vec x) $$\n",
        "\n",
        "**if and only if** \n",
        "\n",
        "$$ P(\\vec X = \\vec x | T(\\vec X) = t)\\ \\text{does not depend on $\\theta$.} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCqaOdhCJKHx",
        "colab_type": "text"
      },
      "source": [
        "_Proof_ (only if direction)\n",
        "\n",
        "First, let's suppose that $p_\\theta(\\vec x)$ has the stated factorization. We need to show that the conditional probability of the data, given $T \\equiv T(\\vec X)$, does not depend on $\\theta$.\n",
        "\n",
        "\\begin{align}\n",
        "P(\\vec X = \\vec x | T = t) &= \\frac{P(\\vec X = \\vec x \\cap T = t)}{P(T = t)} \\\\\n",
        "&= \\begin{cases} \\displaystyle \\frac{P(\\vec X = \\vec x)}{P(T = t)} & T(\\vec x) = t \\\\ 0 & \\text{otherwise} \\end{cases} \n",
        "\\end{align}\n",
        "\n",
        "In the case where $T(\\vec x) \\neq t$, the probability is $0$, which clearly does not depend on $\\theta$. So let's just focus on the $T(\\vec x) = t$ case.\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{P(\\vec X = \\vec x)}{P(T = t)} &= \\frac{P(\\vec X = \\vec x)}{\\sum_{\\vec x: T(\\vec x) = t} P(\\vec X = \\vec x)} \\\\\n",
        "&= \\frac{g(T(\\vec x), \\theta) \\cdot h(\\vec x)}{\\sum_{\\vec x: T(\\vec x) = t} g(T(\\vec x), \\theta) \\cdot h(\\vec x)} & \\text{(by assumption)} \\\\\n",
        "&= \\frac{g(t, \\theta) \\cdot h(\\vec x)}{\\sum_{\\vec x: T(\\vec x) = t} g(t, \\theta) \\cdot h(\\vec x)} & \\text{(since $T(\\vec x) = t$)} \\\\\n",
        "&=  \\frac{g(t, \\theta) \\cdot h(\\vec x)}{g(t, \\theta) \\sum_{\\vec x: T(\\vec x) = t} h(\\vec x)} & \\text{($g(t, \\theta)$ does not depend on $\\vec x$)} \\\\\n",
        "&= \\frac{ h(\\vec x)}{\\sum_{\\vec x: T(\\vec x) = t} h(\\vec x)} & \\text{(cancel out $g(t, \\theta)$)}\n",
        "\\end{align}\n",
        "\n",
        "All terms involving $\\theta$ have vanished. So the conditional probability does not depend on $\\theta$, as we wanted to show."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77iNbmKAv6w2",
        "colab_type": "text"
      },
      "source": [
        "_Proof_ (if direction)\n",
        "\n",
        "Now, suppose that $P(\\vec X = \\vec x | T = t)$ does not depend on $\\theta$. Can we find functions $g$ and $h$ so that $p_\\theta(\\vec x)$ has the stated factorization?\n",
        "\n",
        "\\begin{align} \n",
        "p_\\theta(\\vec x) &= P(\\vec X = \\vec x) \\\\\n",
        "&= \\sum_t P(T = t) P(\\vec X = \\vec x | T = t) & \\text{(Law of Total Probability)} \\\\\n",
        "&= \\underbrace{P(T = T(\\vec x))}_{g(T(\\vec x), \\theta)} \\cdot \\underbrace{P(\\vec X = \\vec x | T = T(\\vec x))}_{h(\\vec x)} & \\text{(only one value of $t$ is consistent with $\\vec x$)}\n",
        "\\end{align}\n",
        "\n",
        "Now, by assumption, the second term $P(\\vec X = \\vec x | T = T(\\vec x))$ does not depend on $\\theta$, so we set this to be $h$. The first term depends on $\\theta$ and the value of $T(\\vec x)$, so we set this to be $g$."
      ]
    }
  ]
}