{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Maximum Likelihood from I.I.D. Data",
      "provenance": [],
      "collapsed_sections": [
        "FhHnELyNFi7G",
        "Ws61O60mM4sC",
        "uW4Tn9vrDPhu",
        "avjrf0VFROch",
        "GREBomanULSu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlsun/Stat425F19/blob/master/Maximum_Likelihood_from_I_I_D_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MkEt4Rwqmy3",
        "colab_type": "text"
      },
      "source": [
        "# Maximum Likelihood from I.I.D. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhHnELyNFi7G",
        "colab_type": "text"
      },
      "source": [
        "## Motivating Example\n",
        "\n",
        "I tossed a (biased) coin 100 times and it came up heads $T = 60$ times. We saw that the MLE of $p$ (based on the observed data $T=60$) is $\\hat p = 60 / 100$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8DPzfVAQj0O",
        "colab_type": "text"
      },
      "source": [
        "What if I told you the exact sequence of heads and tails?\n",
        "$$ X_1 = 1, X_2 = 0, X_3 = 0, ..., X_{100} = 1. $$\n",
        "Remember that $\\sum_{i=1}^{100} X_i = 60$. This is more information than simply knowing that the total number of heads is 60. Does this change your estimate of $p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws61O60mM4sC",
        "colab_type": "text"
      },
      "source": [
        "## General Problem\n",
        "\n",
        "So far, we have estimated parameters $\\theta$ from a single observation $X$ by maximizing the likelihood\n",
        "\n",
        "$$ L_x(\\theta) = p_\\theta(x). $$\n",
        "\n",
        "However, in general, we observe multiple independent observations $X_1, ..., X_n$ from a probability distribution. That is, $X_1, ..., X_n$ are _independent and identically distributed_ (_i.i.d._) from a distribution with p.m.f. $p_\\theta(x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMxcDxnzreVF",
        "colab_type": "text"
      },
      "source": [
        "How do we come up with an estimate based on the values $x_1, ..., x_n$?\n",
        "\n",
        "The idea is the same. We maximize the likelihood of the data, except now the likelihood is the joint probability of the data.\n",
        "\n",
        "\\begin{align}\n",
        "L_{x_1, x_2, ..., x_n}(\\theta) &= p_{\\theta}(x_1, x_2, ..., x_n) \\\\\n",
        "&= p_{\\theta}(x_1) \\cdot p_\\theta(x_2) \\cdot ... \\cdot p_\\theta(x_n) & \\text{(by independence)}\n",
        "\\end{align}\n",
        "\n",
        "We can express this compactly using product notation (which works just like summation notation, escept for products instead of sums):\n",
        "\n",
        "$$ L(\\theta) = \\prod_{i=1}^n p_\\theta(x_i). $$\n",
        "\n",
        "As before, we find the value of $\\theta$ that maximizes $L(\\theta)$, usually by first taking the logarithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW4Tn9vrDPhu",
        "colab_type": "text"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Let $X_1, X_2, ..., X_{n}$ be i.i.d. $\\text{Binomial}(n=1, p)$, also known as _Bernoulli trials_. Find the MLE of $p$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvJ4L4GJSD8Y",
        "colab_type": "text"
      },
      "source": [
        "Let's write down the likelihood and simplify as much as possible:\n",
        "\n",
        "\\begin{align} \n",
        "L(p) &= \\prod_{i=1}^n p_p(X_i) \\\\\n",
        "&= \\prod_{i=1}^n \\binom{1}{X_i} p^{X_i} (1 - p)^{1 - X_i} \\\\\n",
        "&= \\prod_{i=1}^n p^{X_i} (1 - p)^{1 - X_i} & \\left( \\binom{1}{x} = 1 \\text{ for $x=0, 1$} \\right) \\\\\n",
        "&= p^{\\sum X_i} (1 - p)^{\\sum (1 - X_i)} \\\\\n",
        "&= p^{\\sum X_i} (1 - p)^{n - \\sum X_i}.\n",
        "\\end{align}\n",
        "\n",
        "The log-likelihood is then:\n",
        "\n",
        "$$ \\ell(p) = \\left(\\sum_{i=1}^n X_i\\right) \\log p + \\left(n - \\sum_{i=1}^n X_i\\right) \\log (1 - p). $$\n",
        "\n",
        "The score equation is:\n",
        "\n",
        "$$ \\ell'(p) = \\frac{\\sum_{i=1}^n X_i}{p} - \\frac{n - \\sum_{i=1}^n X_i}{1 - p}. $$ \n",
        "\n",
        "Solving for $p$, we find that \n",
        "\n",
        "$$ \\hat p = \\frac{\\sum_{i=1}^n X_i}{n}. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cXtoWCdUJHo",
        "colab_type": "text"
      },
      "source": [
        "So we see that the MLE only depends on $T = \\sum_{i=1}^n X_i$, the total number of heads. The exact sequence of heads and tails does not matter for determining the MLE. In other words, knowing $T$ is _sufficient_ for calculating the MLE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avjrf0VFROch",
        "colab_type": "text"
      },
      "source": [
        "## Sufficient Statistics\n",
        "\n",
        "We did not have to go through the entire calculation to know that the MLE $\\hat p$ only depends on $T = \\sum_{i=1}^n X_i$. We can tell just from the likelihood $L(p)$, or equivalently, the joint p.m.f.\n",
        "\n",
        "We saw that the joint p.m.f. was \n",
        "\n",
        "$$ p^{\\sum X_i} (1 - p)^{n - \\sum X_i}. $$\n",
        "\n",
        "If we replace $\\sum X_i$ (which depends on 100 numbers) by the single number $T$, we get\n",
        "\n",
        "$$ p^{T} (1 - p)^{n - T}. $$\n",
        "\n",
        "When we take logs, derivatives, etc., the resulting expression will only depend on the data through $T$. We no longer need the original values $X_1, X_2, ..., X_{100}$. Knowing $T$ is sufficient. For this reason, $T$ is called a **sufficient statistic**.\n",
        "\n",
        "The theorem below generalizes this observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GREBomanULSu",
        "colab_type": "text"
      },
      "source": [
        "### Theorem and Factorization Definition of Sufficiency\n",
        "\n",
        "Suppose the joint p.m.f. can be factored as \n",
        "\n",
        "$$ p_\\theta(x_1, ..., x_n) = g(T(x_1, ..., x_n), \\theta) \\cdot h(x_1, ..., x_n) $$\n",
        "\n",
        "for some functions $g$, $h$, and $T$. \n",
        "\n",
        "Then, the MLE of $\\theta$ will only depend on the data $X_1, ..., X_n$ through the value of $T \\equiv T(X_1, ..., X_n)$, and $T$ is called a **sufficient statistic** for $\\theta$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CklEw3INYH7P",
        "colab_type": "text"
      },
      "source": [
        "**Proof**\n",
        "\n",
        "We just need to show that the MLE only depends on $T$. Remember that the likelihood is just the joint p.m.f.:\n",
        "\n",
        "$$ L(\\theta) = g(T(X_1, ..., X_n), \\theta) \\cdot h(X_1, ..., x_n). $$\n",
        "\n",
        "Next we calculate the log likelihood.\n",
        "\n",
        "$$ \\ell(\\theta) = \\log g(T(X_1, ..., X_n), \\theta) + \\log h(X_1, ..., X_n). $$\n",
        "\n",
        "Now we take the derivative with respect to $\\theta$. The part of the likelihood that depended on the raw data $X_1, ..., X_n$ will disappear.\n",
        "\n",
        "$$ \\ell'(\\theta) = \\frac{\\partial}{\\partial \\theta} g(T, \\theta) + 0 $$\n",
        "\n",
        "Now, the only data-dependent quantity is $T$, so when we set this expression equal to zero and solve for $\\theta$, it will only depend on $T$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQXLbJI6W8g6",
        "colab_type": "text"
      },
      "source": [
        "## Sufficiency as Dimensionality Reduction\n",
        "\n",
        "To visualize sufficiency, suppose we have two Bernoulli trials $X_1$ and $X_2$. The values of the sufficient statistic $T = X_1 + X_2$ are shown in the table below.\n",
        "\n",
        "|$X_1$ | 0     | 1 |\n",
        "|---|-------|---|\n",
        "|$X_2$ |    |    |\n",
        "| **0** | 0     | 1 |\n",
        "| **1** | 1     | 2 |\n",
        "\n",
        "Sufficiency allows us to reduce two dimensions $(X_1, X_2)$ to one dimension $T$, represented by the diagonal of this table.\n",
        "\n",
        "In general, sufficiency will allow us to reduce $n$ dimensions to just a few dimensions. This can be useful if we are concerned about storage."
      ]
    }
  ]
}