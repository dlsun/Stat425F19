{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numerical Maximum Likelihood",
      "provenance": [],
      "collapsed_sections": [
        "Drc3Fdpy9XhI",
        "-gu2s0cF9ZYc",
        "oLN0FrSRfuul",
        "ByiaAqV1p0aR",
        "7RFcDBEjHLty"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlsun/Stat425F19/blob/master/Numerical_Maximum_Likelihood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inBJHq82dLqh",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Import Symbulate\n",
        "!pip install -q symbulate\n",
        "from symbulate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9in-QMNbcvL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define `plot_continuous_function`\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_continuous_function(f, xlim=(0, 1), xlabel=r\"$\\theta$\", ylabel=\"Likelihood\"):\n",
        "  xs = np.linspace(xlim[0], xlim[1], 1000)\n",
        "  ys = [f(x) for x in xs]\n",
        "  plt.plot(xs, ys, \"-\")\n",
        "  plt.xlabel(xlabel, fontsize=18)\n",
        "  plt.ylabel(ylabel, fontsize=18)\n",
        "  plt.xlim(*xlim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drc3Fdpy9XhI",
        "colab_type": "text"
      },
      "source": [
        "# Numerical Maximum Likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gu2s0cF9ZYc",
        "colab_type": "text"
      },
      "source": [
        "## Motivating Example\n",
        "\n",
        "Let $X_1, ..., X_n$ be i.i.d. $\\text{Gamma}(r, \\lambda=1)$. What is the MLE of $r$?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "108RfiaGbNWm",
        "colab_type": "text"
      },
      "source": [
        "First, we write down the likelihood and simplify as much as possible:\n",
        "\\begin{align}\n",
        "L_{\\vec x}(r) &= \\prod_{i=1}^n \\frac{1}{\\Gamma(r)} x_i^{r-1} e^{-x_i} \\\\\n",
        "&= \\frac{1}{\\Gamma(r)^n} \\left(\\prod_{i=1}^n x_i \\right)^{r-1} e^{-\\sum_{i=1}^n x_i}.\n",
        "\\end{align}\n",
        "\n",
        "Now let's calculate the log likelihood:\n",
        "$$ \\ell(r) = -n\\log \\Gamma(r) + (r-1) \\log \\prod_{i=1}^n x_i - \\sum_{i=1}^n x_i. $$\n",
        "\n",
        "Finally, we take the derivative to obtain the score equation:\n",
        "$$ \\ell'(r) = -n \\frac{\\Gamma'(r)}{\\Gamma(r)} + \\log \\prod_{i=1}^n x_i = 0. $$\n",
        "\n",
        "Solving this equation by hand is impossible. Remember that $\\Gamma(r)$ has a complicated definition: $\\int_0^\\infty t^{r-1} e^{-t}\\,dt$. Now we have to take its derivative?!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHx6LKMgcl-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simulate some data from a gamma distribution,\n",
        "# where r=4.5.\n",
        "X = RV(Gamma(shape=4.5, rate=1))\n",
        "x = X.sim(10)\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLiMxrczbpn0",
        "colab_type": "text"
      },
      "source": [
        "Now let's graph the log-likelihood for this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwHHSR0T9VOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.special import gamma\n",
        "\n",
        "def log_likelihood(r):\n",
        "  n = len(x)\n",
        "  return -n * log(gamma(r)) + (r-1) * sum(log(x)) - sum(x)\n",
        "\n",
        "plot_continuous_function(log_likelihood, \n",
        "                         xlim=(0, 10),\n",
        "                         xlabel=\"$r$\",\n",
        "                         ylabel=\"Log Likelihood\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Tf8lQFeXoh",
        "colab_type": "text"
      },
      "source": [
        "We can find the MLE by eyeballing it. But can we make a computer do it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLN0FrSRfuul",
        "colab_type": "text"
      },
      "source": [
        "# Newton's Method\n",
        "\n",
        "Newton's method is a technique for finding the roots of a function. That is, it is a way to find values $x$ such that\n",
        "\n",
        "$$ f(x) = 0. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-GkMO9hpvFw",
        "colab_type": "text"
      },
      "source": [
        "For example, suppose \n",
        "\n",
        "$$ f(x) = x (2 x - 1), $$\n",
        "\n",
        "graphed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm4l3Df4d3-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "  return x * (2 * x - 1)\n",
        "\n",
        "plot_continuous_function(f)\n",
        "plt.hlines(y=0, xmin=0, xmax=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjb3jhNUgpaj",
        "colab_type": "text"
      },
      "source": [
        "We know that its roots should be $x=0$ and $x=1/2$. But what if we did not know this? Could we start with a blind guess and improve our guess until we get the right answer?\n",
        "\n",
        "For example, suppose we start with the guess $x_0 = 0.8$. Clearly, $f(0.8) \\neq 0$. How would we improve on this guess? We can find a tangent line to the function at $x=0.8$ and find where that tangent line crosses the $x$-axis. This procedure is shown graphically below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EimfNWQgMsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "  return x * (2 * x - 1)\n",
        "\n",
        "def df(x):\n",
        "  return 4 * x - 1\n",
        "\n",
        "plot_continuous_function(f)\n",
        "plt.hlines(y=0, xmin=0, xmax=1)\n",
        "\n",
        "plt.vlines(x=0.8, ymin=0, ymax=f(0.8), linestyles=\"dashed\")\n",
        "\n",
        "xs = np.arange(0.4, 1.1, step=0.1)\n",
        "plt.plot(xs, f(0.8) + df(0.8) * (xs - 0.8), 'r-')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW23bYsSiFyr",
        "colab_type": "text"
      },
      "source": [
        "Note that the equation of this tangent line is \n",
        "\n",
        "$$ t(x) = f(x_0) + f'(x_0) (x - x_0). $$\n",
        "\n",
        "We find where this tangent line crosses the $x$-axis. In other words, we find $x$ such that \n",
        "\n",
        "$$ t(x) = 0. $$\n",
        "\n",
        "The solution is \n",
        "\n",
        "$$ x = x_0 - \\frac{f(x_0)}{f'(x_0)}. $$\n",
        "\n",
        "This value becomes our updated guess $x_1$. Now, we repeat this process:\n",
        "\n",
        "- Find the tangent line to the curve at $x_t$.\n",
        "- Find where this tangent line crosses the $x$-axis.\n",
        "- This location is the new guess $x_{t+1}$.\n",
        "\n",
        "This produces iterates $x_2$, $x_3$, ... that are successively closer to the true root of $0.5$.\n",
        "\n",
        "Let's run 10 iterations of Newton's method, starting with an initial guess of $x=0.8$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyFE5QoZhBDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initial guess\n",
        "x = 0.8\n",
        "\n",
        "# 10 iterations of Newton's method\n",
        "for i in range(10):\n",
        "  print(\"Iteration %d: %.10f\" % (i, x))\n",
        "  x = x - f(x) / df(x)\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6m6Xmf2lDpU",
        "colab_type": "text"
      },
      "source": [
        "Newton's method is remarkable. It converges to the right answer extremely quickly, usually within 5 iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByiaAqV1p0aR",
        "colab_type": "text"
      },
      "source": [
        "# Applying Newton's Method to MLEs\n",
        "\n",
        "Use Newton's method to find the MLE of $r$ in the gamma model, using the data you generated above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlq2hWsckaQV",
        "colab_type": "text"
      },
      "source": [
        "Remember that the MLE solves the score equation \n",
        "$$ \\ell'(r) = 0. $$\n",
        "So we apply Newton's method to the function $f \\equiv \\ell'$. (Since Newton's method also requires $f'$, we will also need to calculate $\\ell''$.)\n",
        "\n",
        "_Hint:_ The [digamma function](https://en.wikipedia.org/wiki/Digamma_function) will come in handy when you implement the score $\\ell'$. For the derivative of the score $\\ell''$, you'll need the derivative of the digamma function, which is provided by the [polygamma function](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.special.polygamma.html). (Note that `polygamma(0, x)` is equivalent to `digamma(x)`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MbIrS7qkLFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.special import gamma, digamma, polygamma\n",
        "\n",
        "# We use the data x defined above.\n",
        "# Define n to be the number of observations.\n",
        "n = len(x)\n",
        "\n",
        "def score(r):\n",
        "  # TODO: Implement the score: the derivative of the log-likelihood.\n",
        "  # This is analogous to f above. It's the function we want the root of.\n",
        "  return r\n",
        "\n",
        "def d_score(r):\n",
        "  # TODO: Implement the derivative of the score.\n",
        "  # This is analogous to df above. It is the slope of the tangent line.\n",
        "  return 1\n",
        "\n",
        "# Run 10 iterations of Newton's method\n",
        "r = 1 # Start with an initial bad guess of r.\n",
        "for i in range(10):\n",
        "  # TODO: Implement Newton's method.\n",
        "  continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RFcDBEjHLty",
        "colab_type": "text"
      },
      "source": [
        "# Newton's Method in Higher Dimensions\n",
        "\n",
        "Newton's method is not necessary for finding roots of one-dimensional functions, since we can simply graph the function and find the root by zooming in on the graph.\n",
        "\n",
        "The power of Newton's method is fully realized for multidimensional function, when the function is difficult or even impossible to visualize. In higher dimensions, Newton's method tries to find $\\vec x$ such that \n",
        "\n",
        "$$ \\vec f(\\vec x) = \\vec 0. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRUNaFF3KEbN",
        "colab_type": "text"
      },
      "source": [
        "Recall that in one dimension, Newton's method says that we should start with an initial guess $x$ and repeatedly update $x$ as follows:\n",
        "\n",
        "$$ x \\leftarrow x - f'(x)^{-1} f(x). $$\n",
        "\n",
        "This rule generalizes to higher dimensions as follows. The \"derivative\" $f'$ becomes a matrix of partial derivatives $[D\\vec f(\\vec x)]$, called [the Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant), and the update rule is \n",
        "\n",
        "$$ \\vec x \\leftarrow \\vec x - [D\\vec f(\\vec x)]^{-1} \\vec f(\\vec x). $$\n",
        "\n",
        "The inverse is a matrix inverse, and the multiplication is matrix multiplication.\n",
        "\n",
        "You will see an application and an implementation of this multivariate Newton's method on the lesson for tomorrow, when we apply this to logistic regression."
      ]
    }
  ]
}